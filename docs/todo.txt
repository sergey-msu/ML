
- GLOBAL REFACTORING in Metric methods!!!
  - rewrite Predict() method in more efficient way: use one walk through TrainingSample
  - do not remove CalculateClassScore method (Gamma_Y(x) calculation), it may be useful as API
  - rewrite CalculateMargins() method in more efficient way
  - do we need Metric.Sort() then?

- serialization
- refactor CNN layers to use Inputs and Outputs
- COMMIT A WILLFUL DECISION: do we need ComputingLayer architecture? Does Build/DoBuild implement in a proper way?
- extract ProximityCalculator from Algorithm.CalculateProximity

***************************** CURRENT *****************************

- CNN:
  - stochastic gradient training (add shiffling)
  - batch normalization https://arxiv.org/pdf/1502.03167.pdf
  - GPU parallelization
  - SSE (Vector class) parallelization
  - Read training set from hard drive not in-memory!
  - extract stop criteria from backpropalgorithm
  - serialization/deserialization
  - compare Keras and ML calcuations
  - inspect convnet.js
  - add kernel_constraint to layers
  - add SoftmaxLayer

- NN and CNN:
  - possibility of: train -> add nodes/layers -> train etc...
  - possibility to remove layer/node from network

- implement more NN optimizators (https://habrahabr.ru/post/318970/)
  - Nesterov

- Unit tests
  - fix NN tests
  - cover CNN with tests
  - cover NN Backprops with tests
  - cover CNN Backprop with tests

- implement different CNN architectures
  - [FUTURE] LeNet-5
  - AlexNet
  - U-Net
  - test with CIFAR-10 images db: http://www.cs.toronto.edu/~kriz/cifar.html

- integration tests: use data files

- demos project (+data samples)

- implement RBF algorith (https://en.wikipedia.org/wiki/Radial_basis_function_network, http://mccormickml.com/2013/08/15/radial-basis-function-network-rbfn-tutorial/)


- [INPROG] Play with ACCORD Framework
  - [INPROG] compare with ML (write benchmarks in .Tests assembly)

- add ability to update parameters in a batch from some start index (extend TryUpdateParams)

- General NN architecture
  - review architecture: public/internal members - try to use assembly from ouside
  - Read https://tproger.ru/translations/neural-network-zoo-1/ - NN zoo
  - add ability to loop nodes
  - develop way to create simple base networks (factory methods)
  - add network-wide global context

 - General ML architcture review
   - review/rewrite Algorithm architecture
     (may be extract Result:TModel property in AlgorithmBase,
      general Train() method that fills the Result in protected abstract DoTrain() method)

 - Cover all algorithm logic with unit tests

 - Implement more algorithms
   - implement C4.5 algorithm for decision trees

 - Implement Boosting (Gradient etc.)

***************************** DONE *****************************

- refactor masking in AlgorithmBase

- CNN:
  - Batch CPU parallelization
  - batch learning
  - dropout
  - add activation layer
  - split 'size' into 'height' and 'width' in all layers
  - use only cats and dogs
  - logistic softmax: check + gradient unittests

- implement Backpropagation algorithm for general full-connected NN
  - implement algorithm
  - add start weight randomization
  - add events to NN algorithms
  - add batching

- add neuron NN + unit tests
- neuron UseBias -> explicit neuron.BiasWeight value
- always UseBias. Get rid of use_bias! fix tests

- implement more NN optimizatiors (https://habrahabr.ru/post/318970/)
  - Momentum
  - Adagrad
  - RMSProp
  - Adadelta
  - Adam
  - Adamax

- implement Convolution NN based on CN architecture
  - read more about Convolution NN
  - implement base CNN layer types
  - implement CNN
  - implement learning algorithm for NN
  - testing with real images
      - MNIST images db: http://yann.lecun.com/exdb/mnist/index.html, https://web.archive.org/web/20160117040036/http://yann.lecun.com/exdb/mnist/)

- implement Perceptron algorithm (no hidden layers)
  - refactor UseBias ( -> neurons threshold as additional parameter?) + cover with tests
  - Perceptron algorithm
  - backpropagation learning
  - batch learning

- Computing Networks/Layers: develop unified approach to multi-typed layers NN: Layer<TIn, TOut>
  - develop architecture
  - refactor: index
  - cover with unit tests
  - develop benchmark tests on parameters get/set/bulk
  - refactor: extract abstract base Layer class
    (may be just mix Hidden and Output logic with 'if' statement? - so there will be only 2 classes: ComputingNetwork and CompositeNetwork)
  - do we REALLY need linked-list architecture? Should we use more evident layered array architecture (with reflection type checks on AddLayer())?

- rewrite all NN code according to new CN architecture
  - rewrite NN
  - cover with tests
  - FlatNeuron and SparseNeuron added to support full-conencted NN along with the sparse ones

